{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Decision trees are popular machine learning algorithms used for classification and regression tasks. They provide a simple and intuitive way to make decisions by breaking down complex problems into a series of simpler decisions.\n",
    "\n",
    "In a decision tree:\n",
    "- Each internal node represents a decision based on a feature.\n",
    "- Each leaf node represents the outcome or prediction.\n",
    "\n",
    "The process of constructing a decision tree involves selecting the best features and splitting the data based on those features to create branches. The goal is to maximize the similarity of the target variable within each branch, resulting in effective predictions.\n",
    "\n",
    "Pros:\n",
    "- Easy to interpret and explain.\n",
    "- Handles both categorical and numerical data.\n",
    "- Robust to missing values and outliers.\n",
    "\n",
    "Cons:\n",
    "- Prone to overfitting, especially with deep trees.\n",
    "- May not generalize well on unseen data.\n",
    "\n",
    "To address overfitting, techniques like pruning or using ensemble methods (e.g., Random Forests) can be applied.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the Data and Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nothing for now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purity and Impurity\n",
    "\n",
    "In the context of decision trees, purity and impurity are essential concepts used to determine the effectiveness of a split at each internal node. The goal of splitting the data is to create branches that are as homogenous as possible with respect to the target variable.\n",
    "\n",
    "- **Purity**: A node is considered pure if all the data points within that node belong to the same class (in the case of classification) or have the same target value (in the case of regression).\n",
    "\n",
    "- **Impurity**: A node is impure if it contains data points from different classes or with different target values. Impurity reflects the uncertainty or randomness in the data at that specific node.\n",
    "\n",
    "## Entropy\n",
    "\n",
    "Entropy is a measure of impurity commonly used in decision trees. For classification tasks, it quantifies the uncertainty associated with the distribution of classes in a dataset. The entropy of a node is given by the formula:\n",
    "\n",
    "$$H(p_1) = -p_1 \\text{log}_2(p_1) - (1- p_1) \\text{log}_2(1- p_1)$$\n",
    "\n",
    "\n",
    "Where:\n",
    "- `p1` represents the proportion of data points belonging to class 1 in the node.\n",
    "\n",
    "Entropy ranges from 0 to 1:\n",
    "- If the node is pure (contains data from only one class), the entropy is 0 (log2(1) = 0).\n",
    "- If the node is equally distributed between class 1 and class 0, the entropy is 1 (log2(0.5) = 1).\n",
    "\n",
    "During the construction of the decision tree, the algorithm searches for the splits that reduce entropy the most, i.e., the splits that create branches with the highest possible purity. A split with low entropy indicates that the resulting child nodes are more homogenous and better separate the data based on the target variable.\n",
    "\n",
    "In summary, entropy is a fundamental concept used in decision trees to measure impurity, and by minimizing entropy, the algorithm creates a tree that makes more informed and accurate decisions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Gain\n",
    "\n",
    "Information Gain is a crucial concept in decision trees that helps determine the best feature to split the data at each internal node. It measures the reduction in entropy achieved by making a particular split. The goal is to find the split that maximizes the information gain, as this will result in the most informative decision.\n",
    "\n",
    "The formula for Information Gain is:\n",
    "\n",
    "$$\\text{Information Gain} = H(p_1^\\text{node})- \\left(w^{\\text{left}}H\\left(p_1^\\text{left}\\right) + w^{\\text{right}}H\\left(p_1^\\text{right}\\right)\\right),$$\n",
    "\n",
    "Where:\n",
    "- `H(p1_node)` is the entropy of the current node before the split.\n",
    "- `H(p1_left)` is the entropy of the left child node after the split.\n",
    "- `H(p1_right)` is the entropy of the right child node after the split.\n",
    "- `w_left` and `w_right` are the weights representing the proportion of data points that go to the left and right child nodes, respectively.\n",
    "\n",
    "The Information Gain quantifies the reduction in uncertainty (entropy) achieved by the split. A higher Information Gain indicates that the split provides more valuable information for making decisions and results in more homogenous child nodes.\n",
    "\n",
    "Decision tree algorithms evaluate the Information Gain for all possible splits on all features and choose the one with the highest gain to make the decision. This process is repeated recursively for each internal node until a stopping criterion, such as reaching a maximum depth or the node becoming pure, is met.\n",
    "\n",
    "By selecting features that lead to the highest Information Gain, the decision tree can efficiently and effectively split the data, resulting in a tree that is powerful for both prediction and interpretation.\n",
    "\n",
    "In summary, Information Gain is a fundamental concept in decision trees that drives the process of feature selection and split creation, allowing the algorithm to make informative and accurate decisions while constructing the tree.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
